# -*- coding: utf-8 -*-
"""sabyasachi_chowdhury_capstone_final_submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YrnJtJncMs2_W4mFWpgL_IxyjigjULny

# Problem Understanding
#### In this section we shall try to Define the problem statement,Need of the study/project, Understanding business/social opportunity

## Background :

Credit cards have become an essential part of our lives. They offer a convenient way to make purchases, build our credit history, and earn rewards. In India, the credit card industry is growing rapidly. In 2022, there were an estimated 70 million credit cardholders in India, and this number is expected to reach 100 million by 2025.

There are many benefits to using credit cards. They offer a convenient way to make purchases, both online and offline. They also help to build your credit history, which can be helpful when you apply for loans or other forms of credit in the future. Additionally, many credit cards offer rewards programs that can help you save money on your everyday purchases.

**The Growth of the Credit Card Industry in India**

The growth of the credit card industry in India is being driven by a number of factors, including:

* The rise of e-commerce: The growth of e-commerce has led to an increase in the demand for credit cards. Credit cards are a convenient way to make online purchases, and they offer a level of security that is not always available with other payment methods.
* The increase in disposable income: The Indian economy is growing, and this is leading to an increase in disposable income. This means that people have more money to spend, and they are increasingly using credit cards to do so.
* The growing acceptance of credit cards: Credit cards are becoming more widely accepted in India. This is making it easier for people to use them, and it is also leading to an increase in the number of people who are applying for credit cards.

**The Issue of Credit Card Default**

Despite the many benefits of credit cards, there is also the risk of credit card default. This occurs when a cardholder fails to make the minimum payment on their credit card bill for a specified period of time. If a cardholder defaults on their credit card, they may face a number of consequences, including:

* Damage to their credit score: A credit card default can damage a cardholder's credit score, making it more difficult to get approved for future loans or credit cards.
* Collection fees: If a cardholder defaults on their credit card, the credit card company may charge them collection fees. These fees can add up, and they can make it even more difficult to pay off the debt.
* Legal action: In some cases, the credit card company may take legal action against a cardholder who defaults on their debt. This could result in wage garnishment or even bankruptcy.


**Conclusion**

Credit cards can be a convenient and useful way to make purchases. However, it is important to use them responsibly and to avoid credit card default. By following the tips above, cardholders can help to protect their credit score and their financial health.

credit:
Reserve Bank of India
https://www.rbi.org.in/
National Consumer Credit Protection Bureau
https://www.consumerfinance.gov/
Experian
https://www.experian.com/

## Problem statement

The problem at hand is a classic example of a supervised learning task in the domain of credit risk assessment for a credit card company. The primary objective is to predict the probability of default, i.e., whether a customer will be able to pay their credit card bill or not. This prediction will be based on a variety of variables related to the customer's credit card account, purchase behavior, and delinquency information.

Predicting default is a crucial aspect of risk management for any lending institution, whether they provide secured or unsecured loans. In this context, 'default' refers to a situation where a customer is unable to meet the legal obligations (or conditions) of a loan, while 'delinquency' refers to a situation where a customer is late or overdue on a payment. Understanding the likelihood of default helps the company assess the level of risk associated with each customer and make informed decisions about lending policies and practices.

This problem falls under the category of 'Classification' in machine learning, as the output we are trying to predict - whether a customer will default or not - is a discrete and finite value. In other words, we are trying to classify customers into two groups: those who are likely to default on their payments and those who are not.

To solve this problem, we will use a supervised learning approach, where our model will be trained on a dataset with known outcomes (i.e., whether each customer in the training set defaulted or not). By learning from this data, the model will be able to predict outcomes for new, unseen data.

### Need of the study/project

The need for this study arises from the inherent risk associated with lending in the financial sector. When a credit card company issues a card to a customer, it essentially extends a line of credit to that individual, trusting that they will repay the borrowed amount. However, not all customers uphold this trust; some default on their payments, leading to financial loss for the company.

By developing a model that can accurately predict the likelihood of a customer defaulting on their credit card payment, the company can proactively manage its risk. This could involve adjusting credit limits, increasing monitoring of certain accounts, or even declining to issue a card in the first place. Therefore, this project is of significant importance to the credit card company's bottom line.

### Understanding Business/Social Opportunity:

From a business perspective, this project presents an opportunity to improve the credit card company's risk management practices, potentially leading to reduced financial losses and increased profitability. By identifying customers who are likely to default, the company can take preventive measures to mitigate risk.

Moreover, the insights gained from this project could also inform the company's customer service and retention strategies. For example, if certain behaviors or characteristics are strongly associated with default, the company could implement targeted interventions or offer personalized financial advice to help those customers manage their credit better.

From a social perspective, this project can contribute to more responsible lending practices. By identifying and understanding the factors that contribute to credit default, lenders can make more informed and fair decisions about who should receive credit. This could help prevent individuals from falling into a cycle of debt, contributing to better financial health and stability for customers.

# Data Report
### In this section we shall analyze how data was collected in terms of time, frequency and methodology, Visual inspection of data (rows, columns, descriptive details), Understanding of attributes (variable info)
"""

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns # for making plots with seaborn
color = sns.color_palette()
import sklearn.metrics as metrics

import warnings
warnings.filterwarnings("ignore")

#loading the data into dataframe
df=pd.read_csv(r"D:\DSBA\DSBA_Great Learning\46_capstone\my project\Banking_Project\PN1_submissions\PD_modelling_dataset.csv")

#general information of the dataframe
df.info()

"""The dataset provided consists  of 99,977 customer records with 36 features. The features include the customer's user ID, default status, account balance, payment history, and other demographic information. The data is a mix of numerical and categorical data.

Some of the key features in the data include:

User ID: A unique identifier for each customer.
Default: A binary variable indicating whether the customer has defaulted on their loan.
Account balance: The customer's current account balance.
Payment history: The customer's payment history over the past 12 months.
Demographic information: The customer's age, gender, and marital status.
"""

df.describe().T

"""Here are some additional observations about the data:

The features have a wide range of values, from very small to very large.
The standard deviation is often much larger than the mean, indicating that the data is not normally distributed.
There are a few outliers in the data, which can skew the results of some statistical analyses.

## Null value in the data
"""

df.isnull().mean()*100

df.duplicated().sum()

unique_counts = {col: df[col].nunique() for col in df.columns}
unique_counts

"""### Conclusion:
Overall, the dataset provided contains information about credit card users and their account details. It has 99978 rows and 36 columns. Each row represents a unique user, and each column represents a specific attribute or feature related to the user's credit card account.

Here's a brief overview of the data collection and attributes:

A. **Data Collection**: The data seems to have been collected over a period of time, with some variables indicating activities over the past 24 months. The frequency of data collection is not explicitly mentioned, but it can be inferred that data was collected at regular intervals, given the time-bound nature of many variables (e.g., 'acct_days_in_dc_12_24m', 'acct_days_in_rem_12_24m', etc.). The methodology of data collection is not provided, but it's likely that this information was gathered from the credit card company's internal records or databases.

B. **Visual Inspection & Descriptive Details**: Upon visual inspection, the dataset contains both numerical and categorical variables. Numerical variables include continuous variables such as 'acct_amt_added_12_24m', 'age', etc., and categorical variables include 'merchant_category', 'merchant_group', etc. The dataset also contains missing values, with the highest percentage of missing values in the 'acct_incoming_debt_vs_paid_0_24m' column (around 59.33%).

C. **Understanding of Attributes**:
The dataset includes the following key attributes:

1. `userid`: The unique user id of the customer who is holding the credit card.
2. `default`: Target Variable. 1 - Indicates the user has defaulted. 0 - Indicates that the person has not defaulted.
3. `acct_amt_added_12_24m`: The total amount of the purchases made using the credit card between 24 months in the past to the present date to the 12 months in the past to the current date.
4. `acct_days_in_dc_12_24m`: The total number of days that the Credit Card Account has stayed in the Debt-Collection Status between 24 months in the past to the present date to the 12 months in the past to the current date.
5. `acct_days_in_rem_12_24m`: The total number of days that the Credit Card Account has stayed in the Reminder Status between 24 months in the past to the present date to the 12 months in the past to the current date.
6. `acct_days_in_term_12_24m`: The total number of days that the Credit Card Account has stayed in the Termination Status between 24 months in the past to the present date to the 12 months in the past to the current date.
7. `acct_incoming_debt_vs_paid_0_24m`: The ratio of the amount collected out of the total debt in an account by an agency to the total debt amount of the account in the previous 24 months from the current date.
8. `acct_status`: The current status of the account. 1 represents active account, while 0 represents inactive account.
9. `acct_worst_status_0_3m`: The total number of days that the Credit Card Account has stayed in the Worst Status between 3 months in the past to the present date.
10. `acct_worst_status_12_24m`: The total number of days that the Credit Card Account has stayed in the Worst Status between 24 months in the past to the present date and 12 months in the past to the current date.
11. `acct_worst_status_3_6m`: The total number of days that the Credit Card Account has stayed in the Worst Status between 6 months in the past to the present date and 3 months in the past to the current date.
12. `acct_worst_status_6_12m`: The total number of days that the Credit Card Account has stayed in the Worst Status between 12 months in the past to the present date and 6 months in the past to the current date.
13. `age`: The age of the customer.
14. `avg_payment_span_0_12m`: The average payment span that the customer has taken in days after the credit card bill got generated in the last one year.
15. `avg_payment_span_0_3m`: The average payment span that the customer has taken in days after the credit card bill got generated in the last three months.
16. `merchant_category`: The category of the merchant.
17. `merchant_group`: The group of the merchant.
18. `has_paid`: Whether the customer has paid the current credit card bill or not. True - Paid. False - Unpaid.
19. `max_paid_inv_0_12m`: The maximum credit card bill amount that has been paid by the customer in the last one year.


20. `max_paid_inv_0_24m`: The maximum credit card bill amount that has been paid by the customer in the last two years.
21. `name_in_email`: Name of the customer in email.
22. `num_active_div_by_paid_inv_0_12m`: Ratio of the number of unpaid bills to the paid bills in the last one year.
23. `num_active_inv`: Number of the active invoices (unpaid bills).
24. `num_arch_dc_0_12m`: Number of archived purchases that were in debt collection status in the last one year.
25. `num_arch_dc_12_24m`: Number of archived purchases that were in debt collection status between 24 months in the past to the present date and 12 months in the past to the current date.
26. `num_arch_ok_0_12m`: Number of archived purchases that were paid in the last one year.
27. `num_arch_ok_12_24m`: Number of archived purchases that were paid between 24 months in the past to the present date and 12 months in the past to the current date.
28. `num_arch_rem_0_12m`: Number of archived purchases that were in the reminder status in the last one year.
29. `status_max_archived_0_6_months`: Maximum number of times the account was in archived status in the last 6 months.
30. `status_max_archived_0_12_months`: Maximum number of times the account was in archived status in the last one year.
31. `status_max_archived_0_24_months`: Maximum number of times the account was in archived status in the last two years.
32. `recovery_debt`: The total amount that has been recovered out of the entire debt amount on the account.
33. `sum_capital_paid_acct_0_12m`: Sum of principal balance paid on account in the last one year.
34. `sum_capital_paid_acct_12_24m`: Sum of principal balance paid on account between 24 months in the past to the present date and 12 months in the past to the current date.
35. `sum_paid_inv_0_12m`: The total amount of the paid invoices in the last one year.
36. `time_hours`: The total hours spent by the customer in purchases made using the credit card.

Each of these attributes provides valuable insights into the customer's credit card usage and payment behavior, which will be instrumental in predicting the likelihood of default.


Overall, this dataset provides a comprehensive view of the credit card users' behavior and their account status, which can be useful for predicting default risk and understanding factors influencing credit card default. However, further data preprocessing, including handling missing values and possibly outlier treatment, will be necessary to make this dataset ready for analysis or modeling.
"""



"""# Exploratory Data Analysis

### In this section we shall do the following :

****Missing Value****
The first step in EDA is to check for missing values. Missing values can occur for a variety of reasons, such as data entry errors or incomplete records. It is important to identify and treat missing values before proceeding with further analysis.

There are several columns with missing values. Depending on the nature of  data we shall try to do these:

   - **Deletion**: If the number of missing values in a column is very high, it might be best to completely drop that column. For example, `acct_incoming_debt_vs_paid_0_24m`, `acct_status`, `acct_worst_status_0_3m`, `acct_worst_status_12_24m`, `acct_worst_status_3_6m`, and `acct_worst_status_6_12m` have a significant number of missing values.

   - **Imputation**: For columns where fewer data is missing, we may consider imputation, which is the process of replacing missing data with substituted values. The strategy for imputation depends on the nature of the data. For numerical data, common strategies include mean, median, or mode imputation. For categorical data, we can use the most frequent category i.e. mode

****Data Type Conversion****
The `userid` column is currently of type float64, but it might be more appropriate to convert it to an integer or string type, as user IDs are typically categorical rather than numerical values.

 **Feature Encoding**: The `merchant_category`, `merchant_group`, and `name_in_email` columns are of object type, which means they likely contain categorical data. For various machine learning algorithms it is needed to encode these categorical variables into a format that can be better understood by the algorithms. Common strategies include one-hot encoding and label encoding.

 ****Outlier Treatment****
Outliers are data points that fall outside the expected range of values. Outliers can be caused by a number of factors, such as data entry errors or unusual events. It is important to identify and treat outliers before proceeding with further analysis.

From the `describe()` output, it seems like there might be some outliers in  data (for example, the max value of `default` is 10000, which can't be true as it is binary data type).

****Check for Correlation and address the problem of Multi collinearity****
Correlation is a measure of the linear relationship between two variables. Multicollinearity is a condition where two or more independent variables are highly correlated. Multicollinearity can cause problems with statistical models, such as reducing the accuracy of the estimates and making the model unstable.

There are a number of ways to check for multicollinearity. One common approach is to calculate the correlation coefficient between the independent variables. If the correlation coefficient is high, then there is a risk of multicollinearity. Another approach is to use a statistical test, such as the variance inflation factor (VIF).

If multicollinearity is present, then there are a number of ways to address the problem. One common approach is to remove one of the correlated variables from the model. Another approach is to transform the data to reduce the correlation between the variables. The best approach to address multicollinearity will depend on the specific dataset and the analysis that is being performed.

****Univariate Analysis****
Univariate analysis can be used to describe the distribution of a variable, identify outliers, and check for normality.

There are a number of ways to perform univariate analysis. One common approach is to use descriptive statistics, such as the mean, median, standard deviation, and range. Another approach is to use graphical techniques, such as histograms, box plots, and cumulative distribution functions.

****Bivariate Analysis****
Bivariate analysis can be used to determine the relationship between two variables, such as whether there is a correlation between the variables or whether one variable predicts the other variable.

There are a number of ways to perform bivariate analysis. One common approach is to use correlation analysis. Another approach is to use graphical techniques, such as scatter plots and line charts.

By conducting a thorough EDA, we can gain a better understanding of the data and identify any potential problems that may need to be addressed before proceeding with further analysis.

## Missing value treatment
"""

missing_values_percent = round(df.isnull().mean() * 100,2)
missing_values_percent

"""Given the percentage of missing data in the DataFrame:

1. **Columns with more than 20% missing data**: These columns have a significant amount of missing data, which could introduce bias if we attempt to impute these missing values. Therefore, it might be best to drop these columns from the dataset. The columns that fall into this category are:

   - 'acct_incoming_debt_vs_paid_0_24m' (59.33% missing)
   - 'avg_payment_span_0_3m' (49.31% missing)
   - 'acct_status' (54.39% missing)
   - 'acct_worst_status_0_3m' (54.39% missing)
   - 'acct_worst_status_12_24m' (66.78% missing)
   - 'acct_worst_status_3_6m' (57.72% missing)
   - 'acct_worst_status_6_12m' (60.36% missing)
   - 'num_active_div_by_paid_inv_0_12m' (29.93% missing)

2. **Columns with less than 20% but more than 0% missing data**: For these columns, imputation could be a good strategy. For numeric data, we can use the median value for imputation, as it is less sensitive to outliers than the mean. For non-numeric data, we can use the mode (the most frequent value) for imputation. The columns that fall into this category are:

   - 'acct_days_in_dc_12_24m' (11.84% missing)
   - 'acct_days_in_rem_12_24m' (11.84% missing)
   - 'acct_days_in_term_12_24m' (11.84% missing)
   - 'default' (10.00% missing)- However, we shall go ahead dropping the respective rows to avoid misleading our model.
   - 'has_paid' (11.04% missing)
   - 'max_paid_inv_0_12m' (11.04% missing)
   - 'max_paid_inv_0_24m' (11.04% missing)
   - 'name_in_email' (11.04% missing)
   - 'num_active_inv' (11.04% missing)
   - 'num_arch_dc_0_12m' (11.04% missing)
   - 'num_arch_dc_12_24m' (11.04% missing)
   - 'num_arch_ok_0_12m' (11.04% missing)
   - 'num_arch_ok_12_24m' (11.04% missing)
   - 'num_arch_rem_0_12m' (11.04% missing)
   - 'status_max_archived_0_6_months' (11.04% missing)
   - 'status_max_archived_0_12_months' (11.04% missing)
   - 'status_max_archived_0_24_months' (11.04% missing)
   - 'recovery_debt' (11.04% missing)
   - 'sum_capital_paid_acct_0_12m' (11.04% missing)
   - 'sum_capital_paid_acct_12_24m' (11.04% missing)
   - 'sum_paid_inv_0_12m' (11.04% missing)
   - 'time_hours' (11.04% missing)

3. **Columns with no missing data**: These columns don't require any handling of missing values. The columns that fall into this category are:

   - 'userid'



"""

#dropping rows with missing value in default column
df =df[df['default'].notnull()]

#Modified shape of the dataframe
df.shape

"""****Lets check once if any of these variables(with more than 20% missing value) are important before dropping****"""

columns=['acct_status','acct_incoming_debt_vs_paid_0_24m','acct_worst_status_0_3m',
         'acct_worst_status_3_6m','acct_worst_status_6_12m',"acct_worst_status_12_24m",
         'avg_payment_span_0_12m','avg_payment_span_0_3m','num_active_div_by_paid_inv_0_12m' ]

# For default = 0
df_default_0 = df[df['default'] == 0]

# For default = 1
df_default_1 = df[df['default'] == 1]


from scipy.stats import ttest_ind

# List to store the results
significant_vars = []

# Perform t-test for each column
for col in columns:
    if df[col].dtype.kind in 'biufc':
        t_stat, p_val = ttest_ind(df_default_0[col], df_default_1[col], equal_var=False)
        if p_val < 0.05:
            significant_vars.append((col, p_val))

# Print the variables for which there is a significant difference between the means
for var, p_val in significant_vars:
    print(f'{var}: p-value = {p_val}')
# Print total numbers of significant variables
print(f'total numbers of significant variables are {len(significant_vars)}')

"""Since none of the variables are significant we may drop these variables

"""

# Dropping the  columns with more than 20% missing data as imputing so much data could bias the model
df = df.drop(columns=['acct_status','acct_incoming_debt_vs_paid_0_24m','acct_worst_status_0_3m','acct_worst_status_3_6m','acct_worst_status_6_12m',"acct_worst_status_12_24m",'avg_payment_span_0_12m','avg_payment_span_0_3m','num_active_div_by_paid_inv_0_12m' ])

# Separating numeric and non-numeric columns
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
non_numeric_cols = list(set(df.columns) - set(numeric_cols))

# Applying median imputation to numeric columns
for col in numeric_cols:
    df[col] = df[col].fillna(df[col].median())

# Replacing missing values in non-numeric columns with the most frequent value
for col in non_numeric_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

# Checking the result
df.isnull().sum()

"""#### Thus we have successfully dropped columns with high amount of missing data and have imputed columns with less than 20% missing data"""

# Dropping the rows where 'default' is 10000 as it is wrong data
df = df[df['default'] != 10000]

# Checking the result
df['default'].value_counts(normalize=True)

"""****Thus only 1.43% of the borrowers are defaulters.****"""

#We also drop userid and name_in_email column as these do not seem to add much relevance in model building
df = df.drop(['userid', 'name_in_email'], axis=1)

# Checking the result
df.columns

"""### Conclusion of the missing value handling steps"""

#shape of the original dataset given :
df1=pd.read_csv(r"D:\DSBA\DSBA_Great Learning\46_capstone\my project\Banking_Project\PN1_submissions\PD_modelling_dataset.csv")
print('shape of the original dataframe :')
display(df1.shape)
# checking the shape of the modified dataframe :
print('shape of the modified dataframe :')
display(df.shape)

# checking the difference in the shape of through imputation:
print(f'Thus through imputation we have dropped {df1.shape[0] - df.shape[0]} columns and {df1.shape[1] - df.shape[1]} rows  :')

print(f'overall we are dealing with {round((df.shape[0]*df.shape[1])/(df1.shape[0]*df1.shape[1]),3)*100}% of the original data provided.It will always be better to enquire with the business and get the null data filled in as due to missing fields in critical columns we are losing important information which could be useful in our model building')

"""## Data type conversion

Firstly, we will focus on cleaning up the 'has_paid' column. This involves standardizing the values to lowercase for consistency, and then transforming the 'true' and 'false' values into a binary format (0 and 1). This transformation is necessary as machine learning models require numerical input. We will also ensure to remove any anomalies present in this column.

Following this, we will optimize our data representation by converting certain float columns to integer type. Specifically, we will identify columns where all non-null values are integers and convert these columns to integer type. This simplifies our data and can help reduce memory usage.

Lastly, we will be converting the categorical 'merchant_category' and 'merchant_group' columns into a numerical format using a method known as label encoding. This again is a necessary step as machine learning models require numerical input.

#### Cleaning 'has_paid' Column:
We will standardize the values by converting them to lowercase and then transform the 'true' and 'false' values into a binary format (0 and 1). We will also remove any anomalies in this column.
"""

# Check the value counts again
df['has_paid'].value_counts()

# Convert all values to lowercase
df['has_paid'] = df['has_paid'].astype(str).str.lower()

# Replace 'true' with 0 and 'false' with 1
df['has_paid'].replace({'true': 0, 'false': 1}, inplace=True)


# Check the value counts again
df['has_paid'].value_counts()

"""#### Converting Float Columns to Integer:
Next, we will identify columns that are of float type but contain integer values. We will convert these columns to integer type to simplify our data and reduce memory usage.
"""

float_cols = df.select_dtypes(include=['float64']).columns
integer_cols = []

for col in float_cols:
    if df[col].dropna().apply(float.is_integer).all():
        integer_cols.append(col)

for col in integer_cols:
    df[col] = df[col].fillna(0).astype(int)

"""# Feature Encoding

#### Label encoding-'merchant_category', 'merchant_group'
Finally, we will convert the categorical 'merchant_category' and 'merchant_group' columns into a numerical format using label encoding. This is a necessary step as machine learning models require numerical input.
"""

# Importing necessary library
from sklearn.preprocessing import LabelEncoder

# Creating a label encoder object
le = LabelEncoder()

# Label Encoding 'merchant_category' column
le.fit(df['merchant_category'])
merchant_category_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
df['merchant_category'] = le.transform(df['merchant_category'])

# Label Encoding 'merchant_group' column
le.fit(df['merchant_group'])
merchant_group_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
df['merchant_group'] = le.transform(df['merchant_group'])

print("Merchant Category Mapping: ", merchant_category_mapping)
print("                                                 ")
print("Merchant Group Mapping: ", merchant_group_mapping)

"""# outlier treatment"""

# Function to calculate the percentage of outliers in each column
def calculate_outlier_percentage(df):
    outlier_percentages = {}
    for col in df.columns:
        if df[col].dtype.kind in 'biufc':
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outlier_step = 1.5 * IQR
            outlier_count = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].shape[0]
            outlier_percentage = round((outlier_count / df.shape[0]) * 100,2)
            outlier_percentages[col] = outlier_percentage
    return outlier_percentages

# Calculate the percentage of outliers in each column
outlier_percentages = calculate_outlier_percentage(df)

# Sort the outlier percentages dictionary by values
sorted_outlier_percentages = sorted(outlier_percentages.items(), key=lambda x: x[1])
# Print the sorted outlier percentages
for col, percentage in sorted_outlier_percentages:
    print(f"{col}: {percentage}%")

"""In the given data, several variables have a high percentage of outliers, such as 'status_max_archived_0_12_months' (42.48%), 'sum_capital_paid_acct_12_24m' (22.55%), and 'num_arch_rem_0_12m' (21.1%). These outliers, if not treated, could lead to less accurate models. Therefore, it's necessary to apply appropriate outlier treatment methods to ensure the robustness of the machine learning models.






"""

# List of categorical and binary variables- these should not be treated
cat_bin_vars = ['acct_worst_status_0_3m', 'acct_worst_status_12_24m', 'acct_worst_status_3_6m',
                'acct_worst_status_6_12m', 'merchant_category', 'merchant_group', 'has_paid',
                'status_max_archived_0_6_months', 'status_max_archived_0_12_months', 'status_max_archived_0_24_months',
                'acct_days_in_dc_12_24m','acct_days_in_rem_12_24m','acct_days_in_term_12_24m','num_arch_dc_0_12m','num_arch_dc_12_24m','num_arch_rem_0_12m',
                'sum_capital_paid_acct_12_24m',"recovery_debt"
               ]

# Apply log transformation to the numeric columns
df_transformed = df.copy()

for col in df.columns:
    if col not in cat_bin_vars  and col != 'default':
        df_transformed[col] = np.log1p(df[col])

# Calculate the percentage of outliers in each column after transformation
outlier_percentages_transformed = calculate_outlier_percentage(df_transformed)
outlier_percentages_transformed

# Sort the outlier percentages dictionary by values
sorted_outlier_percentages = sorted(outlier_percentages_transformed.items(), key=lambda x: x[1])
# Print the sorted outlier percentages
for col, percentage in sorted_outlier_percentages:
    print(f"{col}: {round(percentage,2)}%")

"""*Log transformation didn't reduce outliers much.We shall go ahead with capping the values within 1.5*IQR range*"""

# List of categorical and binary variables.Also ignoring columns where there is heavy predominance of one value( ~90%) as adjusting the same could be misleading
cat_bin_vars = ['acct_worst_status_0_3m', 'acct_worst_status_12_24m', 'acct_worst_status_3_6m',
                'acct_worst_status_6_12m', 'merchant_category', 'merchant_group', 'has_paid',
                'status_max_archived_0_6_months', 'status_max_archived_0_12_months', 'status_max_archived_0_24_months',
                'acct_days_in_dc_12_24m','acct_days_in_rem_12_24m','acct_days_in_term_12_24m','num_arch_dc_0_12m','num_arch_dc_12_24m','num_arch_rem_0_12m',
                'sum_capital_paid_acct_12_24m',"recovery_debt"
               ]

# Apply capping to the columns where the outlier issue is not treated
df_capped = df_transformed.copy()
for col in df.columns:
    if col not in cat_bin_vars and col != 'default' :
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_capped[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        df_capped[col] = np.where(df[col] > upper_bound, upper_bound, df[col])

# Calculate the percentage of outliers in each column after capping
outlier_percentages_capped = calculate_outlier_percentage(df_capped)

# Sort the outlier percentages dictionary by values
sorted_outlier_percentages = sorted(outlier_percentages_capped.items(), key=lambda x: x[1])
# Print the sorted outlier percentages
for col, percentage in sorted_outlier_percentages:
    print(f"{col}: {round(percentage,2)}%")

"""##Thus outliers have been greatly treated.However,The 'status_max_archived_0_12_months' column still has a relatively high percentage of outliers. This could be due to several reasons:

Natural Variability: The data in this column might naturally have a high variability, which could lead to a high number of values being classified as outliers based on the IQR method.

Data Errors: There could be errors in the data collection or entry process that have resulted in incorrect values in this column.

Skewed Distribution: The data in this column might be heavily skewed, with a long tail that results in a high number of values being classified as outliers.

## Check for Correlation and address the problem of Multi collinearity
Next we check correlation analysis.We shall try to identify correlation pairs which are having more than 0.8 correlation among themselves. Next we shall drop one of these two to avoid issues related to multicollinearity during model building
"""

# Importing necessary library
import matplotlib.pyplot as plt
import seaborn as sns

# Calculating the correlation matrix
corr = df_capped.corr()

# Creating a heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)
plt.title('Correlation Heatmap')
plt.show()

"""****There are many pairs of variables wich are having significant correlation. Let us now drop one of them****"""

def drop_correlated_features(df, threshold=0.8):
    # Calculate the correlation matrix
    corr = df.corr()

    # Select the upper triangle of the correlation matrix
    corr_triu = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))

    # Identify the pairs of correlated features
    to_drop = [column for column in corr_triu.columns if any(corr_triu[column] > threshold)]

    # Drop the correlated features
    df = df.drop(df[to_drop], axis=1)

    return df

df_capped = drop_correlated_features(df_capped, threshold=0.8)

df_capped.shape

"""## Exploratory Data Analysis
### Univariate analysis
Univariate analysis is the simplest form of data analysis where we examine each variable individually.

### Numerical variables

### Density plot
"""

for column in df_capped.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure(figsize=(10, 5))
    sns.kdeplot(df_capped[column])
    plt.title('Density Plot of ' + column)
    plt.show()

"""### Conclusion :
The density plots provide a visual representation of the distribution of data within each numeric column of your dataset. Here are some general conclusionsregarding  these plots:

1. **Shape of the Distribution**: Most of these plots are skewed to the left or right suggesting that the data has a skewed distribution.

2. **Presence of Outliers**: Presence of long tails extending to the far left or right of the main peak and small secondary peaks indicates the presence of outliers in the data.

3. **Multi-modality**: Most of these plots have more than one peak( e.g sum_capital_paid_acct_0_12m,num_arch_ok_12_24m etc.) which indicates that the data is multi-modal, suggest that there are multiple groups or clusters in the data.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Get a list of categorical columns
categorical_columns = df_capped.select_dtypes(include=['int32']).columns

# Print value counts for each categorical column and create a pie chart
for col in categorical_columns:
    print(f'\\nColumn Name: {col}')
    print(df_capped[col].value_counts())

    # Get top 5 categories
    top_5 = df_capped[col].value_counts().nlargest(5)

    # Pie chart
    fig, ax = plt.subplots()
    ax.pie(top_5, labels = top_5.index, autopct='%1.1f%%', startangle=140, pctdistance=0.85)

    #draw circle
    centre_circle = plt.Circle((0,0),0.70,fc='white')
    fig = plt.gcf()
    fig.gca().add_artist(centre_circle)

    # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.axis('equal')

    # Move the legend to an empty part of the plot
    plt.legend(loc='upper left')

    plt.title(f'Pie chart of {col}')
    plt.show()

"""### Conclusion :
Analysis of pie chart
•	98.6% of the customers are non-defaulters and only 1.4% are defaulters
•	acct_days_in_dc_12_24m- valued almost entirely ‘0’
•	acct_days_in_rem_12_24m- 98.5% valued ‘0’
•	acct_days_in_term_12_24m- valued almost entirely ‘0’
•	56% of all merchants belong to category 22 (Diversified erotic material)
•	54.9% of all merchants belong to group 4 (Erotic Materials)
•	status_max_archived_0_6_months-56.3% are ‘1’ followed by 30% ‘0’
•	status_max_archived_0_12_months-80% is ‘0’

### Histogram
"""

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Set up the figure size
plt.rcParams['figure.figsize'] = (15, 10)

# Make subplots
fig, axes = plt.subplots(nrows = 5, ncols = 5)

# Draw histogram with KDE in each axes
for i, ax in enumerate(axes.flatten()):
    if i < len(df_capped.columns):
        # Select column
        col = df_capped.columns[i]
        # Check if the column is numerical
        if df_capped[col].dtype.kind in 'biufc':
            # Draw histogram with KDE
            sns.histplot(df_capped[col], kde=True, ax=ax)
            # Set title
            ax.set_title(col)

# Remove empty subplots
for i in range(len(df_capped.columns), len(axes.flatten())):
    fig.delaxes(axes.flatten()[i])

# Show the plot
plt.tight_layout()
plt.show()

"""A histogram is a graphical representation that organizes a group of data points into a specified range. The KDE is used for visualizing the probability density of a continuous variable. It represents the data using a continuous probability density curve.

The histograms provide a visual representation of the distribution of each numerical feature. We can observe the central tendency, dispersion, and skewness of each feature's distribution. The KDE overlay on the histograms helps us understand the shape of the distribution.

However, due to a large number of features and subplots, the individual plots might be a bit small and hard to read.

#### Let us now see if there is significant difference in parameter values between defaulters and non-defaulters
To do this we will first create two sets of descriptive statistics, one for default = 0 and one for default = 1. The descriptive statistics will include the mean, standard deviation, minimum, maximum, and other measures of central tendency and dispersion.
"""

# For default = 0
df_default_0 = df_capped[df_capped['default'] == 0]
description_default_0 = df_default_0.describe().T

# For default = 1
df_default_1 = df_capped[df_capped['default'] == 1]
description_default_1 = df_default_1.describe().T

# Print the descriptions
print("Description for default = 0:\n", description_default_0)
print("\nDescription for default = 1:\n", description_default_1)

"""### A few interesting observations :
The table shows the descriptive statistics for two groups of customers: those who defaulted on their loans (default = 1) and those who did not default (default = 0).

****For customers who defaulted:****

The mean acct_amt_added_12_24m is 4031.385578, which is higher than the mean of 2988.910542 for customers who did not default. This means that, on average, customers who defaulted had a higher acct_amt_added_12_24m than customers who did not default.
The mean recovery_debt is 35.945652, which is also higher than the mean of 2.687263 for customers who did not default. This means that, on average, customers who defaulted had a higher recovery_debt than customers who did not default.
The standard deviation for most of the variables is much larger for default = 1 than for default = 0. This suggests that there is more variation in the values of these variables for customers who defaulted.
The minimum value for most of the variables is 0 for default = 0, but not for default = 1. This suggests that there were some customers who had not defaulted had very low values for these variables.
The maximum value for most of the variables is much higher for default = 1 than for default = 0. This suggests that there were some customers who defaulted who had very high values for these variables.

****For customers who did not default:****

The mean acct_amt_added_12_24m is 2988.910542, which is lower than the mean of 4031.385578 for customers who defaulted. This means that, on average, customers who did not default had a lower acct_amt_added_12_24m than customers who defaulted.
The mean recovery_debt is 2.687263, which is also lower than the mean of 35.945652 for customers who defaulted. This means that, on average, customers who did not default had a lower recovery_debt than customers who defaulted.
The standard deviation for most of the variables is much smaller for default = 0 than for default = 1. This suggests that there is less variation in the values of these variables for customers who did not default.
The minimum value for most of the variables is 0 for default = 0, and this is also the case for default = 1. This suggests that there were no customers who had very low values for these variables.
The maximum value for most of the variables is lower for default = 0 than for default = 1. This suggests that there were no customers who did not default who had very high values for these variables.

Overall, the descriptive statistics show that there are some clear differences between the two groups. These differences could be used to build a predictive model that could predict whether a customer is likely to default on their loan.

### Let us observe this visually
"""

# Set up the figure size
plt.rcParams['figure.figsize'] = (15, 10)

# Drop 'default' column from the dataframes
df_default_0 = df_default_0.drop(columns=['default'])
df_default_1 = df_default_1.drop(columns=['default'])

# Make subplots for default = 0
fig, axes = plt.subplots(nrows = 5, ncols = 5)

# Draw histogram with KDE in each axes for default = 0
for i, ax in enumerate(axes.flatten()):
    if i < len(df_default_0.columns):
        # Select column
        col = df_default_0.columns[i]
        # Check if the column is numerical
        if df_default_0[col].dtype.kind in 'biufc':
            # Draw histogram with KDE
            sns.histplot(df_default_0[col], kde=True, ax=ax, color='blue')
            # Set title
            ax.set_title(col)

# Remove empty subplots
for i in range(len(df_default_0.columns), len(axes.flatten())):
    fig.delaxes(axes.flatten()[i])

# Show the plot
plt.tight_layout()
plt.show()

# Make subplots for default = 1
fig, axes = plt.subplots(nrows = 5, ncols = 5)

# Draw histogram with KDE in each axes for default = 1
for i, ax in enumerate(axes.flatten()):
    if i < len(df_default_1.columns):
        # Select column
        col = df_default_1.columns[i]
        # Check if the column is numerical
        if df_default_1[col].dtype.kind in 'biufc':
            # Draw histogram with KDE
            sns.histplot(df_default_1[col], kde=True, ax=ax, color='red')
            # Set title
            ax.set_title(col)

# Remove empty subplots
for i in range(len(df_default_1.columns), len(axes.flatten())):
    fig.delaxes(axes.flatten()[i])

# Show the plot
plt.tight_layout()
plt.show()

"""Thus we can visually confirm that the mean value of age is higher for non-defaulters.The merchant group for non-defaulters is maximum around 4 whereas that for defaulters are having multiple modes around 2,4 and 10.

These are just preliminary observations based on the mean values. To confirm these observations, we would need to perform statistical tests (like t-tests) to see if these differences are statistically significant.

## Bivariate analysis

### Boxplot
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Selecting  columns without the default
columns=df_capped.columns
numeric_columns = columns[columns != 'default']

# Define the subplot structure
num_plots = len(numeric_columns)
num_cols = 5
num_rows = num_plots // num_cols + (num_plots % num_cols > 0)
fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 20))

# Iterate over the columns
for ind, column in enumerate(numeric_columns):
    # Create figure and axes
    row = ind // num_cols
    col = ind % num_cols
    ax = axs[row, col]

    # Box plot
    data = df_capped.copy()
    data['default'] = pd.to_numeric(data['default'], errors='coerce')
    data = data.dropna(subset=['default'])
    if not data.empty:
        sns.boxplot(y=column, x='default', data=data, ax=ax)
        ax.set_title(f'Distribution of Default \n by {column}')
        ax.set_xlabel(column)
        ax.set_ylabel('Value')

# Remove empty subplots
if num_plots % num_cols != 0:
    for ax in axs.flatten()[num_plots:]:
        fig.delaxes(ax)

plt.tight_layout()
plt.show()

"""### Conclusion :
1. The mean age of defaulters is lesser than that of non-defaulters.
2. The mean value of The maximum credit card bill amount that has been paid by the customer in the last one year and last two years are higher for non-defaulters as compared to defaulters, which is understandable.
3. Similarly, The mean total amount of the paid invoices in the last one year is higher for non-defaulter as compared to defaulter.

## Heatmap
"""

# Calculating the correlation matrix
corr = df_capped.corr()

# Creating a heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)
plt.title('Correlation Heatmap')
plt.show()

"""Here are the pairs of variables that have a significant correlation( 0.5):

1. 'acct_amt_added_12_24m' and 'sum_capital_paid_acct_0_12m': 0.652040
   - This indicates a moderate positive correlation. It suggests that the total amount of the purchases made using the credit card between 24 months in the past to the present date to the 12 months in the past to the current date is moderately related to the sum of principal balance paid on account in the last one year. This could be because as the amount of purchases increases, the amount of principal balance paid also increases, reflecting the customer's ability to pay off their debts.

2. 'acct_amt_added_12_24m' and 'sum_capital_paid_acct_12_24m': 0.530793
   - This indicates a moderate positive correlation. It suggests that the total amount of the purchases made using the credit card between 24 months in the past to the present date to the 12 months in the past to the current date is moderately related to the sum of principal balance paid on account between 24 months in the past to the present date and 12 months in the past to the current date. This could be because customers who have a higher amount of purchases also tend to have a higher amount of principal balance paid, indicating their financial capability.

3. 'status_max_archived_0_6_months' and 'status_max_archived_0_12_months': 0.726621
   - This indicates a strong positive correlation. It suggests that the maximum number of times the account was in archived status in the last 6 months is strongly related to the maximum number of times the account was in archived status in the last one year. This could be because customers who frequently have their account in archived status in the short term (6 months) are likely to also have their account in archived status frequently in the longer term (12 months).

4. 'sum_capital_paid_acct_0_12m' and 'sum_capital_paid_acct_12_24m': 0.480283
   - This indicates a moderate positive correlation. It suggests that the sum of principal balance paid on account in the last one year is moderately related to the sum of principal balance paid on account between 24 months in the past to the present date and 12 months in the past to the current date. This could be because customers who are diligent in paying off their principal balance in the short term are also diligent in the longer term.

These correlations suggest that there are some relationships between the variables in the dataset. However, correlation does not imply causation, and these relationships could be influenced by other factors not included in the dataset.

## t-test to verify significant variables
"""

from scipy.stats import ttest_ind

# List to store the results
significant_vars = []

# Perform t-test for each column
for col in df_default_0.columns:
    if df_default_0[col].dtype.kind in 'biufc':
        t_stat, p_val = ttest_ind(df_default_0[col], df_default_1[col], equal_var=False)
        if p_val < 0.05:
            significant_vars.append((col, p_val))

# Print the variables for which there is a significant difference between the means
for var, p_val in significant_vars:
    print(f'{var}: p-value = {p_val}')
# Print total numbers of significant variables
print(f'total numbers of significant variables are {len(significant_vars)}')

"""#### Thus there are around 18 significant variables for which the mean value of the indepenedent parameters are different between the defaulters and non-defaulters

### Two way anova
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Perform two-way ANOVA for merchant_category and has_paid
model1 = ols('default ~ C(merchant_category) + C(has_paid) + C(merchant_category):C(has_paid)', data=df_capped).fit()
anova_table1 = anova_lm(model1, typ=2)

# Perform two-way ANOVA for merchant_group and has_paid
model2 = ols('default ~ C(merchant_group) + C(has_paid) + C(merchant_group):C(has_paid)', data=df_capped).fit()
anova_table2 = anova_lm(model2, typ=2)

# Perform two-way ANOVA for merchant_category and merchant_group
model3 = ols('default ~ C(merchant_category) + C(merchant_group) + C(merchant_category):C(merchant_group)', data=df_capped).fit()
anova_table3 = anova_lm(model3, typ=2)

anova_table1, anova_table2, anova_table3

"""Interpretation of the results:

1. **Merchant Category and Has Paid**: The p-value for `merchant_category` is less than 0.05, indicating that the merchant category has a significant effect on the default status. The p-value for `has_paid` is greater than 0.05, indicating that the payment status does not have a significant effect on the default status. The interaction term between `merchant_category` and `has_paid` is significant (p-value < 0.05), suggesting that the effect of merchant category on default status depends on whether the customer has paid or not.

2. **Merchant Group and Has Paid**: The p-value for `merchant_group` is less than 0.05, indicating that the merchant group has a significant effect on the default status. The p-value for `has_paid` could not be calculated due to some issue (possibly due to a lack of variation in the `has_paid` variable). The interaction term between `merchant_group` and `has_paid` is significant (p-value < 0.05), suggesting that the effect of merchant group on default status depends on whether the customer has paid or not.

3. **Merchant Category and Merchant Group**: The p-values for `merchant_category` and `merchant_group` are greater than 0.05, indicating that neither of these variables has a significant effect on the default status. The interaction term between `merchant_category` and `merchant_group` is not significant (p-value > 0.05), suggesting that the effect of merchant category on default status does not depend on the merchant group, and vice versa.

However, these interpretations are based on the assumption that the assumptions of ANOVA (normality, homogeneity of variances, and independence of observations) are met, which may not be the case here

# Point Biserial Correlation
It is a correlation measure for a continuous and a binary variable. It measures the strength and direction of the association between the two variables.
"""

from scipy.stats import pointbiserialr

# Selecting continuous and binary variables
continuous_vars = df_capped.select_dtypes(include=['float64', 'int64']).columns
binary_var = 'default'

# Calculate Point Biserial Correlation
correlations = {}
for var in continuous_vars:
    corr, _ = pointbiserialr(df_capped[binary_var], df_capped[var])
    correlations[var] = corr

# Display correlations
correlations

"""The result can be described as :

'acct_amt_added_12_24m': 0.024289445566718165: This indicates a very weak positive correlation between the 'acct_amt_added_12_24m' variable and the 'default' variable. This means that as the 'acct_amt_added_12_24m' increases, the likelihood of default slightly increases.

'age': -0.043827496352982664: This indicates a weak negative correlation between the 'age' variable and the 'default' variable. This means that as the 'age' increases, the likelihood of default slightly decreases.

'has_paid': 0.03006043757297283: This indicates a very weak positive correlation between the 'has_paid' variable and the 'default' variable. This means that if the customer has paid, the likelihood of default slightly increases.

'num_active_inv': 0.020724773572789578: This indicates a very weak positive correlation between the 'num_active_inv' variable and the 'default' variable. This means that as the number of active invoices increases, the likelihood of default slightly increases.

'num_arch_ok_12_24m': -0.0716965941704307: This indicates a weak negative correlation between the 'num_arch_ok_12_24m' variable and the 'default' variable. This means that as the number of archived purchases that were paid in the last 24 months increases, the likelihood of default slightly decreases.

'sum_capital_paid_acct_0_12m': 0.022827876361864466: This indicates a very weak positive correlation between the 'sum_capital_paid_acct_0_12m' variable and the 'default' variable. This means that as the sum of principal balance paid on account in the last one year increases, the likelihood of default slightly increases.

'time_hours': -0.008332393688636516: This indicates a very weak negative correlation between the 'time_hours' variable and the 'default' variable. This means that as the total hours spent by the customer in purchases made using the credit card increases, the likelihood of default slightly decreases.

In general, all these correlations are very weak, suggesting that these variables alone may not be strong predictors of default.

### Clustering
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Selecting the features
features = df_capped.drop('default', axis=1)

# Standardizing the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Finding the optimal number of clusters using the Elbow Method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(features_scaled)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

from sklearn.cluster import KMeans

# Let's assume the optimal number of clusters from the Elbow Method is 10
kmeans = KMeans(n_clusters=10, init='k-means++', random_state=42)

# Selecting the features
features = df_capped[['acct_amt_added_12_24m', 'acct_days_in_dc_12_24m', 'acct_days_in_rem_12_24m', 'acct_days_in_term_12_24m', 'age', 'merchant_category', 'merchant_group', 'has_paid', 'max_paid_inv_0_12m', 'num_active_inv', 'num_arch_dc_0_12m', 'num_arch_dc_12_24m', 'num_arch_ok_0_12m', 'num_arch_ok_12_24m', 'status_max_archived_0_6_months', 'recovery_debt', 'sum_capital_paid_acct_0_12m', 'time_hours']]

# Fitting the model
kmeans.fit(features)

# Adding the cluster labels to the original dataframe
df_capped['cluster'] = kmeans.labels_

# Checking the distribution of customers in each cluster
print((df_capped['cluster'].value_counts(normalize=True))*100)

# Comparing the clusters
cluster_comparison = df_capped.groupby('cluster').agg({
    'acct_amt_added_12_24m': 'mean',
    'acct_days_in_dc_12_24m': 'mean',
    'acct_days_in_rem_12_24m': 'mean',
    'acct_days_in_term_12_24m': 'mean',
    'age': 'mean',
    'merchant_category': 'mean',
    'merchant_group': 'mean',
    'has_paid': 'mean',
    'max_paid_inv_0_12m': 'mean',
    'num_active_inv': 'mean',
    'num_arch_dc_0_12m': 'mean',
    'num_arch_dc_12_24m': 'mean',
    'num_arch_ok_0_12m': 'mean',
    'num_arch_ok_12_24m': 'mean',
    'status_max_archived_0_6_months': 'mean',
    'recovery_debt': 'mean',
    'sum_capital_paid_acct_0_12m': 'mean',
    'time_hours': 'mean',
    'default': 'mean'
})
print(cluster_comparison)

"""considering the overall default rate of 1.4% and the distribution of customers in each cluster:

1. Cluster 1 (28.08% of customers): This cluster has a lower default rate (1.0%) than the overall population. This suggests that a large proportion of customers (28.08%) in this cluster, who add less to their accounts and pay less capital, are less likely to default.

2. Cluster 3 (22.39% of customers): This cluster has a higher default rate (2.1%) than the overall population. This suggests that a significant proportion of customers (22.39%) in this cluster, despite being older and having all paid in the past, still have a higher risk of default.

3. Cluster 0 (11.67% of customers): This cluster has a higher default rate (2.4%) than the overall population. This suggests that customers in this cluster, who add a high average account amount in the last 12-24 months and pay a high average sum of capital in the last 0-12 months, have a higher risk of default.

4. Cluster 7 (10.64% of customers): This cluster has a lower default rate (0.7%) than the overall population. This suggests that customers in this cluster, who add less to their accounts and pay less capital, are less likely to default.

5. Cluster 2 (7.63% of customers): This cluster has a significantly lower default rate (0.2%) than the overall population. This suggests that customers in this cluster, who pay large invoices but don't add much to their accounts, are less likely to default.

6. Cluster 4 (5.59% of customers): This cluster has a higher default rate (2.4%) than the overall population. This suggests that a smaller proportion of customers (5.59%) in this cluster, who add a significant amount to their accounts and pay a large sum of capital, have a higher risk of default.

7. Cluster 8 (5.06% of customers): This cluster has a higher default rate (2.0%) than the overall population. This suggests that customers in this cluster, despite adding a moderate amount to their accounts and paying a large sum of capital, still have a higher risk of default.

8. Cluster 5 (4.96% of customers): This cluster has a lower default rate (0.7%) than the overall population. This suggests that customers in this cluster, who pay a large sum of capital and add a significant amount to their accounts, are less likely to default.

9. Cluster 6 (2.24% of customers): This cluster has a lower default rate (0.8%) than the overall population. This suggests that a smaller proportion of customers (2.24%) in this cluster, who add a moderate amount to their accounts and pay a large sum of capital, are less likely to default.

10. Cluster 9 (1.74% of customers): This cluster has a lower default rate (0.6%) than the overall population. This suggests that a small proportion of customers (1.74%) in this cluster, despite adding a significant amount to their accounts, paying a moderate sum of capital, and having a high recovery debt, are less likely to default.

In conclusion, clusters 0, 3, 4, and 8 have a higher default rate than the overall population, indicating a higher risk of default. These clusters are characterized by high average account amounts added in the last 12-24 months and high average sums of capital paid in the last 0-12 months (clusters 0 and 4), older customers and all customers having paid in the past (cluster 3), and moderate account amounts added and large sums of capital paid (cluster 8). Therefore, these factors might indicate a higher risk of default.

On the other hand, clusters 1, 2, 5, 6, 7, and 9 have a lower default rate than the overall population, indicating a lower risk of default. These clusters are characterized by lower account amounts added, lower sums of capital paid, and lower recovery debt (clusters 1, 2, 5, 7, and 9), and moderate account amounts added and large sums of capital paid (cluster 6). Therefore, these factors might indicate a lower risk of default.

It's important to note that the largest clusters (1 and 3) have contrasting default rates. Cluster 1, which comprises 28.08% of customers, has a lower default rate, while Cluster 3, which comprises 22.39% of customers, has a higher default rate. This suggests that there is a significant variation in default risk within the customer base, and this risk cannot be accurately predicted by cluster size alone. Other factors, such as account activity and payment behavior, play a crucial role in determining default risk.
"""



df_capped.info()











"""# Model Building

### Building base model
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
import pandas as pd

# Splitting the data into train and test
X = df_capped.drop('default', axis=1)
y = df_capped['default']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# List of models to evaluate
models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(),  GaussianNB(), BaggingClassifier(), AdaBoostClassifier()]

# Model names
model_names = ['log_reg', 'decision_tree', 'random_forest',  'gaussian_nb', 'bagging', 'adaboost']

# Initialize a dataframe to store the results
results = pd.DataFrame(columns=['Model', 'Accuracy_Train', 'Recall_Train', 'Precision_Train', 'F1_Train',
                                'Accuracy_Test', 'Recall_Test', 'Precision_Test', 'F1_Test'])

# Loop through the models and evaluate each
for i, model in enumerate(models):
    model.fit(X_train, y_train)

"""In the model building phase, we first prepare our data for the modeling process. This involves separating our target variable ('default') from our feature variables (all other columns in the 'df_capped' DataFrame). We then split our data into a training set and a testing set. The training set is used to train our models, while the testing set is used to evaluate the performance of our models on unseen data. The split is done in such a way that 70% of the data goes to the training set and 30% goes to the testing set.

Next, we initialize a list of the models we want to evaluate. In this case, we are evaluating six different models: Logistic Regression, Decision Tree, Random Forest, Gaussian Naive Bayes, Bagging, and AdaBoost. Each of these models is a different machine learning algorithm and they each have their own strengths and weaknesses. By evaluating multiple models, we can compare their performance and choose the best one for our specific task.

We also create a DataFrame to store the results of our model evaluation. This DataFrame will contain the name of each model and the performance metrics for that model on both the training and testing data.

Finally, we loop through each model in our list, fit the model to our training data, and store the fitted model for later use. The fitting process involves the model learning the patterns in the training data so that it can make predictions on new, unseen data.

### Model Interpretation
"""

from sklearn.metrics import classification_report, accuracy_score

for i, model in enumerate(models):
    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Get the classification report for each model (returns a dictionary)
    report_train = classification_report(y_train, y_train_pred, output_dict=True)
    report_test = classification_report(y_test, y_test_pred, output_dict=True)

    # Get accuracy for each model
    accuracy_train = accuracy_score(y_train, y_train_pred)
    accuracy_test = accuracy_score(y_test, y_test_pred)

    # Add the results to the dataframe
    results = results.append({
        'Model': model_names[i],
        'Accuracy_Train': accuracy_train,
        'Recall_Train': report_train['1']['recall'],
        'Precision_Train': report_train['1']['precision'],
        'F1_Train': report_train['1']['f1-score'],
        'Accuracy_Test': accuracy_test,
        'Recall_Test': report_test['1']['recall'],
        'Precision_Test': report_test['1']['precision'],
        'F1_Test': report_test['1']['f1-score']
    }, ignore_index=True)

results

"""### Interpretation of each model's performance:


1. **Logistic Regression (log_reg)**: This model has a high accuracy on both the training and testing data (98.5% and 98.6% respectively). However, the recall, precision, and F1 score are very low, especially on the testing data. This suggests that while the model is good at predicting the majority class, it struggles to correctly identify the minority class (defaulters).

2. **Decision Tree (decision_tree)**: This model performs extremely well on the training data with high accuracy, recall, precision, and F1 score. However, its performance drops significantly on the testing data, suggesting that the model may be overfitting the training data and not generalizing well to unseen data.

3. **Random Forest (random_forest)**: Similar to the Decision Tree model, the Random Forest model performs well on the training data but not as well on the testing data. The drop in performance is not as drastic as the Decision Tree model, but it still suggests some degree of overfitting.

4. **Gaussian Naive Bayes (gaussian_nb)**: This model has lower accuracy compared to the other models, and its recall, precision, and F1 score are also relatively low. This suggests that the model may not be a good fit for this particular dataset.

5. **Bagging (bagging)**: The Bagging model performs well on the training data but its performance drops on the testing data, similar to the Decision Tree and Random Forest models.

6. **AdaBoost (adaboost)**: The AdaBoost model has high accuracy but low recall, precision, and F1 score, similar to the Logistic Regression model. This suggests that the model is good at predicting the majority class but struggles with the minority class.

In summary, all models seem to struggle with correctly identifying the minority class (defaulters), as indicated by the low recall, precision, and F1 scores on the testing data. This could be due to class imbalance in the dataset. The Decision Tree, Random Forest, and Bagging models also appear to be overfitting the training data, as their performance drops significantly on the testing data.

## Model Tuning
"""

from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support
# List of models to evaluate
models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), GaussianNB(), BaggingClassifier(), AdaBoostClassifier()]

# Initialize a dataframe to store the results
results = pd.DataFrame(columns=['Model', 'Accuracy_Train', 'Recall_Train', 'Precision_Train', 'F1_Train',
                                'Accuracy_Test', 'Recall_Test', 'Precision_Test', 'F1_Test', 'Optimal_Threshold'])

# Splitting the data into train and test
X = df_capped.drop('default', axis=1)
y = df_capped['default']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Loop through the models and evaluate each
for i, model in enumerate(models):
    model.fit(X_train, y_train)

    # Make probability predictions
    y_train_pred_prob = model.predict_proba(X_train)[:, 1]
    y_test_pred_prob = model.predict_proba(X_test)[:, 1]

    # Calculate the ROC curve and AUC for each model
    fpr, tpr, thresholds = roc_curve(y_train, y_train_pred_prob)

    # Find the optimal threshold
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]

    # Make predictions based on the optimal threshold
    y_train_pred = (y_train_pred_prob >= optimal_threshold).astype(int)
    y_test_pred = (y_test_pred_prob >= optimal_threshold).astype(int)

    # Get the classification report for each model (returns a dictionary)
    report_train = precision_recall_fscore_support(y_train, y_train_pred, average='binary')
    report_test = precision_recall_fscore_support(y_test, y_test_pred, average='binary')

    # Get accuracy for each model
    accuracy_train = accuracy_score(y_train, y_train_pred)
    accuracy_test = accuracy_score(y_test, y_test_pred)

    # Add the results to the dataframe
    results = results.append({
        'Model': model_names[i],
        'Accuracy_Train': accuracy_train,
        'Recall_Train': report_train[1],
        'Precision_Train': report_train[0],
        'F1_Train': report_train[2],
        'Accuracy_Test': accuracy_test,
        'Recall_Test': report_test[1],
        'Precision_Test': report_test[0],
        'F1_Test': report_test[2],
        'Optimal_Threshold': optimal_threshold
    }, ignore_index=True)

results

"""Thus we can see that the recall value has significantly improved but at the cost of precision values

### Step1 : creating new dataframe after removing columns using VIF(>5)
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd
import numpy as np

# Select only numeric features
numeric_features = df_capped.select_dtypes(include=[np.number])

# Define a function to calculate VIF
def calculate_vif(df):
    vif = pd.DataFrame()
    vif["Features"] = df.columns
    vif["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif

# Calculate VIF for the initial set of features
vif = calculate_vif(numeric_features)

# Drop features with VIF > 5 iteratively
while vif['VIF'].max() > 5:
    # Drop the feature with the highest VIF
    numeric_features = numeric_features.drop([vif['Features'][vif['VIF'].argmax()]], axis=1)

    # Recalculate VIF
    vif = calculate_vif(numeric_features)

vif

"""Here, the VIF values for all the features are less than 5, which is generally considered a good threshold for determining if multicollinearity is a problem. This means that there is low multicollinearity between the features in your dataset.

The feature with the highest VIF is 'status_max_archived_0_6_months' with a VIF of 4.295280, which is still below the threshold of 5. This suggests that while this feature does have some correlation with other features, it is not so high that it is likely to significantly impact your model.

In conclusion, the VIF results suggest that multicollinearity is not a significant problem in your dataset, and therefore, it is unlikely that you need to take any action to address multicollinearity.







"""

vif.Features.unique()

df_capped_vif=df_capped[['default', 'acct_amt_added_12_24m', 'acct_days_in_dc_12_24m',
       'acct_days_in_rem_12_24m', 'acct_days_in_term_12_24m',
       'merchant_category', 'merchant_group', 'has_paid',
       'max_paid_inv_0_12m', 'num_active_inv', 'num_arch_dc_0_12m',
       'num_arch_dc_12_24m', 'num_arch_ok_12_24m', 'num_arch_rem_0_12m',
       'status_max_archived_0_6_months', 'recovery_debt',
       'sum_capital_paid_acct_0_12m', 'sum_capital_paid_acct_12_24m']]
df_capped_vif.default.value_counts(normalize=True)

"""### 2.since the original data is highly imbalanced, we shall try to use upsampling of the default=1 class"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score
from sklearn.utils import resample
from sklearn.preprocessing import StandardScaler

# List of models to evaluate
models = [LogisticRegression(), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5),  GaussianNB(), BaggingClassifier(), AdaBoostClassifier()]
model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'GaussianNB', 'Bagging', 'AdaBoost']

# Initialize a dataframe to store the results
results = pd.DataFrame(columns=['Model', 'Accuracy_Train', 'Recall_Train', 'Precision_Train', 'F1_Train',
                                'Accuracy_Test', 'Recall_Test', 'Precision_Test', 'F1_Test', 'Optimal_Threshold'])

# Splitting the data into train and test
X = df_capped_vif.drop('default', axis=1)
y = df_capped_vif['default']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Concatenate our training data back together
X = pd.concat([pd.DataFrame(X_train), y_train.reset_index(drop=True)], axis=1)

# Separate minority and majority classes
not_default = X[X.default==0]
default = X[X.default==1]

# Upsample minority
default_upsampled = resample(default,
                          replace=True, # sample with replacement
                          n_samples=len(not_default), # match number in majority class
                          random_state=27) # reproducible results

# Combine majority and upsampled minority
upsampled = pd.concat([not_default, default_upsampled])

# Randomly select 50% of the upsampled data
upsampled = upsampled.sample(frac=0.25, random_state=42)

# Check new class counts
upsampled.default.value_counts()

y_train = upsampled.default
X_train = upsampled.drop('default', axis=1)

# Loop through the models and evaluate each
for i, model in enumerate(models):
    model.fit(X_train, y_train)

    # Make probability predictions
    y_train_pred_prob = model.predict_proba(X_train)[:, 1]
    y_test_pred_prob = model.predict_proba(X_test)[:, 1]

    # Calculate the ROC curve and AUC for each model
    fpr, tpr, thresholds = roc_curve(y_train, y_train_pred_prob)

    # Find the optimal threshold
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]

    # Make predictions based on the optimal threshold
    y_train_pred = (y_train_pred_prob >= optimal_threshold).astype(int)
    y_test_pred = (y_test_pred_prob >= optimal_threshold).astype(int)

    # Get the classification report for each model (returns a dictionary)
    report_train = precision_recall_fscore_support(y_train, y_train_pred, average='binary')
    report_test = precision_recall_fscore_support(y_test, y_test_pred, average='binary')

    # Get accuracy for each model
    accuracy_train = accuracy_score(y_train, y_train_pred)
    accuracy_test = accuracy_score(y_test, y_test_pred)

    # Add the results to the dataframe
    results = results.append({
        'Model': model_names[i],
        'Accuracy_Train': accuracy_train,
        'Recall_Train': report_train[1],
        'Precision_Train': report_train[0],
        'F1_Train': report_train[2],
        'Accuracy_Test': accuracy_test,
        'Recall_Test': report_test[1],
        'Precision_Test': report_test[0],
        'F1_Test': report_test[2],
        'Optimal_Threshold': optimal_threshold
    }, ignore_index=True)

results

"""### Summary

In the above code, we are performing the following steps:

1. **Data Preparation**: We first split the data into training and testing sets and scale the features using StandardScaler. This is done to ensure that all features have a similar scale, which can improve the performance of many machine learning algorithms.

2. **Upsampling**: We then balance the dataset by upsampling the minority class (default=1) to match the number of samples in the majority class (default=0). This is done to prevent the model from being biased towards the majority class. We then randomly select 25% of the upsampled data to prevent the model from overfitting to the upsampled minority class.

3. **Model Training and Evaluation**: We train several models (Logistic Regression, Decision Tree, Random Forest, GaussianNB, Bagging, and AdaBoost) on the upsampled training data and evaluate their performance on both the training and testing data. We use several metrics for evaluation, including accuracy, recall, precision, and F1 score. We also calculate the optimal threshold for classifying a sample as the positive class based on the ROC curve.

The results show the performance of each model on the training and testing data. From the results, we can see that the Bagging classifier has the highest accuracy on the training data (90.3%) and the testing data (84.9%). However, its recall on the testing data is relatively low (41.4%), which means it is not very good at identifying the positive class (default=1). On the other hand, the GaussianNB classifier has the highest recall on the testing data (86.5%), but its accuracy on the testing data is relatively low (61.8%).

In conclusion, there is a trade-off between accuracy and recall, and the best model depends on the specific requirements of your task. If it is more important to correctly identify the positive class, then a model with a high recall (like GaussianNB) might be more suitable. If overall accuracy is more important, then a model with a high accuracy (like Bagging) might be more suitable.

### Optimal threshold balancing both recall and precision
"""

from sklearn.metrics import precision_recall_curve

# Calculate precision, recall, and thresholds
precision, recall, thresholds = precision_recall_curve(y_train, y_train_pred_prob)

# Calculate F1 score for each threshold
f1_scores = 2*(precision*recall) / (precision + recall)

# Find the optimal threshold
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

"""In the provided code, we are calculating the precision, recall, and F1 score for different threshold values and identifying the optimal threshold that maximizes the F1 score.
1. **Calculate Precision, Recall, and Thresholds**: The `precision_recall_curve` function from sklearn is used to compute a list of precision and recall values for different thresholds. The thresholds are applied to the predicted probabilities for the positive class (`y_train_pred_prob`) to convert them into class predictions.

2. **Calculate F1 Scores**: For each threshold, the F1 score is calculated. The F1 score is the harmonic mean of precision and recall and is a popular metric for evaluating the performance of binary classifiers, especially in imbalanced datasets. It ranges from 0 to 1, where 1 indicates perfect precision and recall.

3. **Find Optimal Threshold**: The optimal threshold is the one that maximizes the F1 score. The `np.argmax` function is used to find the index of the maximum F1 score, and this index is used to find the corresponding threshold.

The optimal threshold can be used to convert the predicted probabilities into class predictions, and it provides a balance between precision and recall. By adjusting this threshold, you can control the trade-off between precision and recall to suit your specific needs. For example, in a situation where false negatives are more costly, you might choose a lower threshold to increase recall at the expense of precision.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score
from sklearn.utils import resample

# List of models to evaluate
models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(),  GaussianNB(), BaggingClassifier(), AdaBoostClassifier()]
model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'GaussianNB', 'Bagging', 'AdaBoost']

# Initialize a dataframe to store the results
results = pd.DataFrame(columns=['Model', 'Accuracy_Train', 'Recall_Train', 'Precision_Train', 'F1_Train',
                                'Accuracy_Test', 'Recall_Test', 'Precision_Test', 'F1_Test', 'Optimal_Threshold'])

# Splitting the data into train and test
X = df_capped_vif.drop('default', axis=1)
y = df_capped_vif['default']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Concatenate our training data back together
X = pd.concat([X_train, y_train], axis=1)

# Separate minority and majority classes
not_default = X[X.default==0]
default = X[X.default==1]

# Upsample minority
default_upsampled = resample(default,
                          replace=True, # sample with replacement
                          n_samples=len(not_default), # match number in majority class
                          random_state=27) # reproducible results

# Combine majority and upsampled minority
upsampled = pd.concat([not_default, default_upsampled])


# Randomly select 50% of the upsampled data
#upsampled = upsampled.sample(frac=0.25, random_state=42)

# Check new class counts
upsampled.default.value_counts()

y_train = upsampled.default
X_train = upsampled.drop('default', axis=1)

# Loop through the models and evaluate each
for i, model in enumerate(models):
    model.fit(X_train, y_train)

    # Make probability predictions
    y_train_pred_prob = model.predict_proba(X_train)[:, 1]
    y_test_pred_prob = model.predict_proba(X_test)[:, 1]

    # Calculate the ROC curve and AUC for each model
    fpr, tpr, thresholds = roc_curve(y_train, y_train_pred_prob)


    # Make predictions based on the optimal threshold
    y_train_pred = (y_train_pred_prob >= optimal_threshold).astype(int)
    y_test_pred = (y_test_pred_prob >= optimal_threshold).astype(int)

    # Get the classification report for each model (returns a dictionary)
    report_train = precision_recall_fscore_support(y_train, y_train_pred, average='binary')
    report_test = precision_recall_fscore_support(y_test, y_test_pred, average='binary')

    # Get accuracy for each model
    accuracy_train = accuracy_score(y_train, y_train_pred)
    accuracy_test = accuracy_score(y_test, y_test_pred)

    # Add the results to the dataframe
    results = results.append({
        'Model': model_names[i],
        'Accuracy_Train': accuracy_train,
        'Recall_Train': report_train[1],
        'Precision_Train': report_train[0],
        'F1_Train': report_train[2],
        'Accuracy_Test': accuracy_test,
        'Recall_Test': report_test[1],
        'Precision_Test': report_test[0],
        'F1_Test': report_test[2],
        'Optimal_Threshold': optimal_threshold
    }, ignore_index=True)

results

"""The optimal threshold value is the same for all models (0.497259), which was determined by maximizing the F1 score on the training data. This threshold is used to convert the predicted probabilities into class predictions.

Here's a brief interpretation of the results:

1. **Logistic Regression**: The model has a good recall on the test set (0.789474), but the precision is quite low (0.029014), indicating that while the model is good at identifying the positive class, it also produces a lot of false positives. The F1 score, which balances precision and recall, is also low (0.055970), suggesting that the model's performance could be improved.

2. **Decision Tree**: The model is highly accurate on the training set (0.903217) and performs reasonably well on the test set (0.854301). However, the recall on the test set is low (0.327068), indicating that the model misses a significant number of positive instances. The precision on the test set is also low (0.034387), suggesting a high number of false positives.

3. **Random Forest**: Similar to the Decision Tree model, the Random Forest model is highly accurate on the training set but performs less well on the test set. The recall and precision on the test set are both low, indicating that the model may be overfitting the training data.

4. **GaussianNB**: This model has the highest accuracy on the test set (0.961658), but the recall is low (0.172932), indicating that the model misses a large number of positive instances. The precision is relatively high (0.089147), suggesting that when the model predicts a positive instance, it is usually correct.

5. **Bagging**: The Bagging model has a high accuracy on the training set and a reasonable accuracy on the test set. However, similar to the Decision Tree and Random Forest models, the recall and precision on the test set are low, suggesting possible overfitting.

6. **AdaBoost**: The AdaBoost model has a high recall on the test set (0.913534), but the precision is low (0.032028), indicating a high number of false positives. The F1 score is also low (0.061887), suggesting that the model's performance could be improved.

In summary, while some models perform well on the training data, they do not generalize well to the test data, suggesting overfitting. The models could potentially be improved by tuning hyperparameters, using different feature selection methods, or gathering more data.
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score
from sklearn.utils import resample
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score


# List of models to evaluate
models = [LogisticRegression(), DecisionTreeClassifier(), GaussianNB(), BaggingClassifier(), AdaBoostClassifier()]
model_names = ['Logistic Regression', 'Decision Tree', 'GaussianNB', 'Bagging', 'AdaBoost']

# Initialize a dataframe to store the results
results = pd.DataFrame(columns=['Model', 'Accuracy_Train', 'Recall_Train', 'Precision_Train', 'F1_Train',
                                'Accuracy_Test', 'Recall_Test', 'Precision_Test', 'F1_Test', 'Optimal_Threshold'])

# Splitting the data into train and test
X = df_capped_vif.drop('default', axis=1)
y = df_capped_vif['default']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Concatenate our training data back together
X = pd.concat([X_train, y_train], axis=1)

# Separate minority and majority classes
not_default = X[X.default==0]
default = X[X.default==1]

# Upsample minority
default_upsampled = resample(default,
                          replace=True, # sample with replacement
                          n_samples=len(not_default), # match number in majority class
                          random_state=27) # reproducible results

# Combine majority and upsampled minority
upsampled = pd.concat([not_default, default_upsampled])

# Check new class counts
upsampled.default.value_counts()

y_train = upsampled.default
X_train = upsampled.drop('default', axis=1)

# Define min_leaf as 1% of the train data and min split as 3*min_leaf

min_samples_leaf = max(int(0.01 * len(X_train)), 1)  # Adjust the percentage as needed
min_samples_split = 3 * min_samples_leaf


# Initialize the Random Forest Classifier
rf = RandomForestClassifier(random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [min_samples_split],
    'min_samples_leaf': [min_samples_leaf],
    'bootstrap': [True, False]
}

# Initialize the GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')

# Append the grid search and the model name to the lists
models.append(grid_search)
model_names.append('Random Forest Grid Search')


# Fixed threshold value
fixed_threshold = 0.4972593555121882

# Loop through the models and evaluate each
for i, model in enumerate(models):
    model.fit(X_train, y_train)

    # Make probability predictions
    y_train_pred_prob = model.predict_proba(X_train)[:, 1]
    y_test_pred_prob = model.predict_proba(X_test)[:, 1]

    # Make predictions based on the fixed threshold
    y_train_pred = (y_train_pred_prob >= fixed_threshold).astype(int)
    y_test_pred = (y_test_pred_prob >= fixed_threshold).astype(int)

    # Get the classification report for each model (returns a dictionary)
    report_train = precision_recall_fscore_support(y_train, y_train_pred, average='binary')
    report_test = precision_recall_fscore_support(y_test, y_test_pred, average='binary')

    # Get accuracy for each model
    accuracy_train = accuracy_score(y_train, y_train_pred)
    accuracy_test = accuracy_score(y_test, y_test_pred)

    # Add the results to the dataframe
    results = results.append({
        'Model': model_names[i],
        'Accuracy_Train': accuracy_train,
        'Recall_Train': report_train[1],
        'Precision_Train': report_train[0],
        'F1_Train': report_train[2],
        'Accuracy_Test': accuracy_test,
        'Recall_Test': report_test[1],
        'Precision_Test': report_test[0],
        'F1_Test': report_test[2],
        'Optimal_Threshold': fixed_threshold
    }, ignore_index=True)

results

"""In the latest code, the primary new step introduced is the hyperparameter tuning for the Random Forest model. This is an important step in machine learning model building as it helps to optimize the model's performance by searching for the best set of hyperparameters. Here's a brief rundown of the new steps:

1. **Hyperparameter Tuning**: We've introduced GridSearchCV to perform hyperparameter tuning for the Random Forest model. We've defined a grid of hyperparameters to search over, including the number of trees in the forest (n_estimators), the maximum depth of the trees (max_depth), the minimum number of samples required to split an internal node (min_samples_split), the minimum number of samples required to be at a leaf node (min_samples_leaf), and whether bootstrap samples are used when building trees (bootstrap).

2. **Model Training with GridSearchCV**: We've added the Random Forest model with GridSearchCV to our list of models. This means that when we train our models, we're not just training a standard Random Forest model, but a Random Forest model with the best hyperparameters found by GridSearchCV.

3. **Fixed Threshold for Predictions**: Instead of finding an optimal threshold for each model, we've used a fixed threshold to make predictions from the predicted probabilities. This allows us to compare the performance of the models more directly, as they are all using the same threshold for predictions.

These new steps help to improve the performance of our models and provide a more direct comparison between them.

### Result analysis

Looking at the results, we can see that all models have been evaluated based on their performance on both the training and test datasets. The metrics used for evaluation include accuracy, recall, precision, F1 score, and the optimal threshold.

The Decision Tree, Bagging, and Random Forest Grid Search models all have a difference of less than 10% between their training and test accuracies, which is a good sign as it suggests that these models are not overfitting to the training data.

However, when we consider the recall values, the Random Forest Grid Search model stands out. It has a recall of approximately 0.866 on the training data and 0.805 on the test data. Recall is a critical metric in this context as it measures the model's ability to correctly identify the positive class, which in this case is 'default'. A high recall indicates that the model is good at identifying defaults.

The Random Forest Grid Search model also has a reasonable F1 score, which is the harmonic mean of precision and recall, indicating a balance between these two metrics.

Therefore, considering the criteria of a less than 10% difference between train and test accuracy, good recall values on both train and test, and overall good performance, the Random Forest Grid Search model appears to be the best choice. This model has been fine-tuned using GridSearchCV, which means it has been optimized to perform well on this specific dataset.

However, it's important to remember that model performance can vary depending on the specific characteristics of the data. It's always a good idea to re-evaluate model performance if the underlying data changes significantly.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc


# Create a figure
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))

# Loop through the models and plot the ROC curve
for i, model in enumerate(models):
    for j, (X, y, data) in enumerate([(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]):
        # Prediction probabilities for each model
        y_pred_prob = model.predict_proba(X)[:, 1]

        # ROC curve values for each model
        fpr, tpr, _ = roc_curve(y, y_pred_prob)

        # AUC for each model
        roc_auc = auc(fpr, tpr)

        # Plot ROC curve for each model
        ax[j].plot(fpr, tpr, label='%s %s ROC curve (area = %0.2f)' % (model_names[i], data, roc_auc))

    # Plot the random line.
    ax[j].plot([0, 1], [0, 1], 'k--')

    # Set plot labels and title
    ax[j].set_xlim([0.0, 1.0])
    ax[j].set_ylim([0.0, 1.05])
    ax[j].set_xlabel('False Positive Rate')
    ax[j].set_ylabel('True Positive Rate')
    ax[j].set_title('Receiver Operating Characteristic for %s Data' % data)
    ax[j].legend(loc="lower right")

# Display the figure
plt.tight_layout()
plt.show()

"""Interpreting a machine learning model like Random Forest involves understanding the importance of different features in making predictions and how changes in those features can impact the outcome. Here are some steps  to interpret the Random Forest model and its implications for the business:



"""

### Feature importance

# Assuming 'grid_search' is your trained RandomForest model
best_rf_model = grid_search.best_estimator_

# Get feature importances
importances = best_rf_model.feature_importances_

# Convert the importances into one-dimensional 1darray with corresponding df column names as axis labels
f_importances = pd.Series(importances, X_train.columns)

# Sort the array in descending order of the importances
f_importances.sort_values(ascending=False, inplace=True)

# Make the bar Plot from f_importances
f_importances.plot(x='Features', y='Importance', kind='bar', figsize=(16,9), rot=45)

# Show the plot
plt.tight_layout()
plt.show()

f_importances

"""Based on the feature importance results, the top three features that contribute the most to predicting whether a customer will default or not are:

1. **num_arch_ok_12_24m**: This is the number of archived purchases that were paid between 24 months in the past to the present date and 12 months in the past to the current date. This feature is the most important, indicating that the history of a customer's payment behavior over the past 12 to 24 months is a strong predictor of whether they will default. Customers who have a history of making payments are less likely to default.

2. **status_max_archived_0_6_months**: This is the maximum number of times the account was in archived status in the last 6 months. This feature being the second most important suggests that recent account activity, specifically the frequency of the account being in an archived status, is a significant predictor of default. If an account is frequently archived, it may indicate that the customer is struggling to keep up with payments, increasing the likelihood of default.

3. **acct_days_in_rem_12_24m**: This is the total number of days that the Credit Card Account has stayed in the Reminder Status between 24 months in the past to the present date to the 12 months in the past to the current date. This feature being the third most important suggests that the length of time an account has been in the reminder status in the past 12 to 24 months is a significant predictor of default. If a customer's account is often in the reminder status, it may indicate that the customer frequently forgets or neglects to make payments, which could increase the likelihood of default.

From a business perspective, these insights can be very valuable. They suggest that monitoring a customer's payment history, recent account activity, and the frequency and duration of reminder status can help predict whether a customer is likely to default. This information can be used to develop strategies to mitigate the risk of default, such as sending payment reminders to customers who frequently have their accounts in reminder status or offering financial counseling to customers who frequently have their accounts archived.

Based on the results of the feature importance and the data dictionary provided, here are some business implications:

1. **'num_arch_ok_12_24m' (Number of archived purchases that were paid between 24 months in the past to the present date and 12 months in the past to the current date)**: This feature has the highest importance score. It suggests that customers who have a history of paying off their purchases are less likely to default. The business could potentially offer incentives or rewards to customers who consistently pay off their purchases to encourage this behavior.

2. **'status_max_archived_0_6_months' (Maximum number of times the account was in archived status in the last 6 months)**: This feature also has a high importance score. If an account frequently goes into archived status, it could be a sign that the customer is struggling to keep up with their payments. The business might want to reach out to these customers to offer assistance or payment plans.

3. **'acct_days_in_rem_12_24m' (The total number of days that the Credit Card Account has stayed in the Reminder Status between 24 months in the past to the present date to the 12 months in the past to the current date)**: This feature suggests that customers who frequently receive reminders are more likely to default. The business could potentially improve their reminder system or offer additional support to these customers.

4. **'max_paid_inv_0_12m' (The maximum credit card bill amount that has been paid by the customer in the last one year)**: This feature indicates that customers who have made significant payments in the past year are less likely to default. The business could potentially offer incentives or rewards to customers who make large payments.

5. **'num_arch_dc_0_12m' (Number of archived purchases that were in debt collection status in the last one year)**: This feature suggests that customers who have had purchases go into debt collection are more likely to default. The business might want to put more stringent checks on these customers or offer them assistance to prevent default.

In conclusion, the business could potentially reduce defaults by focusing on customers who have shown signs of financial distress, such as frequently going into archived status or receiving many reminders. They could also encourage good payment behavior by rewarding customers who consistently pay off their purchases or make large payments.
"""

